{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (Principal component analysis)\n",
    "\n",
    "Use: Dimensionality reduction\n",
    "\n",
    "why dimensionality reduction ?\n",
    "\n",
    "**EDA:**\n",
    "- visualization\n",
    "-     redundant features\n",
    "\n",
    "The goal of PCA is to find a lower dimensional representation of your features without losing any relevant information.\n",
    "\n",
    "Steps for applying PCA:\n",
    "- Obtain the covariance matrix of the feature matrix\n",
    "- perform eigen decomposition of covariance matrix\n",
    "- use the eigen vectors to project the feature matrix into a new space\n",
    "\n",
    "## --- The Maths ---\n",
    "let X be our feature matrix. \n",
    "$X \\ \\epsilon \\ \\mathbb{R}^{m \\times n}$ \n",
    "\n",
    "for each feature j;<br/>\n",
    "mean,<br/> $\\mu_j = \\frac{1}{m}\\sum_{i=1}^{m} x_j^{(i)}$ \n",
    "\n",
    "variance,<br/> \n",
    "$var_j = \\frac{1}{m}\\sum_{i=1}^{m} (x_j^{(i)} - \\mu_j)^2$\n",
    "\n",
    "for any two features j and k, the covariance is<br/>\n",
    "$covar_{j,k} = \\frac{1}{m}\\sum_{i=1}^{m} (x_j^{(i)} - \\mu_j)(x_k^{(i)} - \\mu_k)$\n",
    "\n",
    "Let's assume a feature matrix of the form ,  $ X =\n",
    "\\begin{bmatrix}x_{1}^{(1)} & x_{2}^{(1)} & x_{3}^{(1)}\\\\x_{1}^{(2)} & x_{2}^{(2)} & x_{3}^{(2)}\\\\x_{1}^{(3)} & x_{2}^{(3)} & x_{3}^{(3)}\\\\x_{1}^{(4)} & x_{2}^{(4)} & x_{3}^{(4)}\\end{bmatrix} $, and $X \\ \\epsilon \\ \\mathbb{R}^{4 \\times 3}$ \n",
    "\n",
    "It's good to perform feature scaling and normalization before applying PCA. This makes each feature column to have a mean, $\\mu$ of 0 and variance $\\sigma^2$ of 1.\n",
    "\n",
    "feature scaling and normalization =>  $X = \\frac{X - \\mu}{\\sigma}$\n",
    "\n",
    "The covariance matrix is one of the form\n",
    "$ C_X =\n",
    "\\begin{bmatrix}var_1 & covar_{1,2} & covar_{1,3}\\\\covar_{2,1} & var_2 & covar_{2,3}\\\\covar_{3,1} & covar_{3,2} & var_3\\end{bmatrix} $, and $C_X \\ \\epsilon \\ \\mathbb{R}^{n \\times n}$ \n",
    "\n",
    "The idea here, is that a feature column should be highly correlated with it self and less correlated with other feature columns. correlation values ranges from -1 to 1. a correlation of 1 means highly positive correlated and -1 means highly negetively correlated while 0 means no correlation at all. Hence a feature column should have a covariance of 1 with it self and 0 with other feature columns.\n",
    "\n",
    "To obtain a representation of the feature matrix, $X'$ with diagonal covariance matrix, $C_X'$, we apply the transformation $X'^T = PX^T$, $X' = XP^T$\n",
    "\n",
    "by the definition of covariance\n",
    "$$C_X = \\frac{1}{m}X^TX$$\n",
    "$$C_X' = \\frac{1}{m}X'^TX'$$\n",
    "$$C_X' = \\frac{1}{m}(XP^T)^T(XP^T)$$\n",
    "$$C_X' = \\frac{1}{m}PX^TXP^T$$\n",
    "$$C_X' = P\\frac{1}{m}X^TXP^T$$\n",
    "$$C_X' = PC_XP^T$$\n",
    "\n",
    "The covariance matrix $C_X$ is symmetric, and $C_X'$ needs to be diagonal. Hence P has to be a matrix of the eigen vectors of C_X at each row.\n",
    "\n",
    "using the property that for a symetric matrix, A, and its orthogonal/orthonomal matrix of eigen vectors $Q$,\n",
    "$Q^TAQ = D$ where $D$ is a diagonal matrix\n",
    "\n",
    "Hence, (if $P^T$ is orthonogonal), $$C_X' = PC_XP^T = D$$\n",
    "for P^T is orthonormal, then, $$C_X' = PC_XP^T = I$$, where $I$ is the unit matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on a toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1,2,4],[1,3,6],[2,6,12],[3,4,8]])\n",
    "m, n = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  4],\n",
       "       [ 1,  3,  6],\n",
       "       [ 2,  6, 12],\n",
       "       [ 3,  4,  8]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last feature column in X is redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply feature scaling and normalization\n",
    "X_norm = (X-X.mean(axis=0))/X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90453403, -1.18321596, -1.18321596],\n",
       "       [-0.90453403, -0.50709255, -0.50709255],\n",
       "       [ 0.30151134,  1.52127766,  1.52127766],\n",
       "       [ 1.50755672,  0.16903085,  0.16903085]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain covariance matrix of X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_X = (1/(m))*np.dot(X_norm.T, X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.56061191, 0.56061191],\n",
       "       [0.56061191, 1.        , 1.        ],\n",
       "       [0.56061191, 1.        , 1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the eigen vectors (Principal components) and values of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, P = np.linalg.eig(C_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.43732141e+00, 5.62678588e-01, 1.27131532e-32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.82993297e-01,  8.75624049e-01,  1.96270488e-17],\n",
       "       [ 6.19159703e-01, -3.41527836e-01, -7.07106781e-01],\n",
       "       [ 6.19159703e-01, -3.41527836e-01,  7.07106781e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.56061191, 0.56061191],\n",
       "       [0.56061191, 1.        , 1.        ],\n",
       "       [0.56061191, 1.        , 1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.90208316e+00,  1.61706169e-02,  1.11022302e-16],\n",
       "       [-1.06482642e+00, -4.45659309e-01,  5.55111512e-17],\n",
       "       [ 2.02945560e+00, -7.75106748e-01, -2.22044605e-16],\n",
       "       [ 9.37453975e-01,  1.20459544e+00,  1.38777878e-17]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.T.dot(X_norm.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.90208316e+00,  1.61706169e-02,  7.50909007e-18],\n",
       "       [-1.06482642e+00, -4.45659309e-01,  4.16986813e-17],\n",
       "       [ 2.02945560e+00, -7.75106748e-01,  1.68335240e-17],\n",
       "       [ 9.37453975e-01,  1.20459544e+00,  2.61579580e-17]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit_transform(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.33333333, 0.74748255, 0.74748255],\n",
       "       [0.74748255, 1.33333333, 1.33333333],\n",
       "       [0.74748255, 1.33333333, 1.33333333]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.get_covariance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90453403, -1.18321596, -1.18321596],\n",
       "       [-0.90453403, -0.50709255, -0.50709255],\n",
       "       [ 0.30151134,  1.52127766,  1.52127766],\n",
       "       [ 1.50755672,  0.16903085,  0.16903085]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The toy example above shows how to apply PCA on the feature matrix of size (4 X 3) and can be extended to all feature matrices of any size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference materials:\n",
    "\n",
    "Sebastian rascha's post on  <a href='https://sebastianraschka.com/Articles/2014_pca_step_by_step.html'>Implementing a Principal Component Analysis (PCA)</a><br/>\n",
    "Rishav Kumar's post on <a href='https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0'>Understanding Principal Component Analysis</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
